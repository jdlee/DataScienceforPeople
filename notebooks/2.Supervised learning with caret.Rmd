---
title: "Supervised learning with caret"
author: "John D. Lee"
date: "2/10/20"
output: html_notebook
---


```{r}

library(tidyverse)
library(skimr)
library(ggforce)
library(caret) # Tools and common interface to many supervised learning algorithms
library(patchwork) # For combining multiple plots
library(plotROC)
library(plotROC)
library(pROC)

set.seed(888) # To ensure consistent results from non-deterministic procedures
rm(list = ls()) # Removes all variables

```


##  Simplified machine learning process

* Read, transform, and visualize data 

* Partition into training and test sets

* Partition data into training and test sets </b>

* Pre-process data and select features

* Tune model hyperparameters with cross validation

* Estimate variable importance 

* Assess predictions and model performance with test data

* Compare model performance

## Read, clean, transform, and visualize data
```{r review_data}

wine.df = read.csv("../data/winequality-red.csv")

skim(wine.df)

head(wine.df)

tail(wine.df)

```

### Define class variable--Good or Bad wine 
```{r define-class, echo = TRUE, warning=FALSE, message=FALSE, out.width="60%"}

quality.wine.df = wine.df %>% mutate(goodwine = if_else(quality > 5, "good", "bad")) %>% 
  mutate(goodwine = as.factor(goodwine))

ggplot(quality.wine.df, aes(goodwine, quality, colour = goodwine, fill = goodwine)) +
  geom_sina(size = .5, alpha = .7, position = position_jitter(height = 0.1)) +
  labs(x = "Discretized wine quality", y = "Rated wine quality") +
  theme(legend.position = "none")

```



## Partition data into training and testing
```{r partition, echo = TRUE}

wine.df = quality.wine.df %>% select(-quality) # Remove numeric indicator of quality

## Creates a random sample of rows for training
inTrain = createDataPartition(wine.df$goodwine, p = 3/4, list = FALSE) 

## Create dataframes of descriptive variables for training and testing
# Slice extracts rows based on vector of row numbers
trainDescr = wine.df %>% slice(inTrain) %>% select(-goodwine)
testDescr = wine.df %>% slice(-inTrain) %>% select(-goodwine)

trainClass = wine.df %>% slice(inTrain) %>% select(goodwine) %>% as.matrix() %>% as.factor()
testClass = wine.df %>% slice(-inTrain) %>% select(goodwine) %>% as.matrix() %>% as.factor()

## Proportion of good and bad cases should be the same in testing and training
# Ideally the classes should be balanced
wine.df %>% select(goodwine) %>%  table() %>% prop.table() %>% round(3)*100 

trainClass %>% table() %>% prop.table() %>% round(3)*100

testClass %>% table() %>% prop.table() %>% round(3)*100 

```


## Pre-process data: Filter poor predictors, select good predictors, and scale
* Eliminate variables that have no variabilty and those that are highly correlated 

* Select highly predictive features

* Engineer features by scaling and transformation

* ``preProcess`` creates a transformation function that can be applied to new data

  * ``center`` subtracts mean

  * ``scale`` normalizes based on standard deviation

  *  ``preProcess`` supports other preprocessing methods, such as PCA and ICA

```{r pre-process, echo = TRUE, cache=FALSE, warning=FALSE, message=FALSE}

## Trans.mod is a transformation model that is trained and the applied to the data
Trans.mod = preProcess(trainDescr, method = c("center", "scale")) 
trainScaled = predict(Trans.mod, trainDescr)
testScaled = predict(Trans.mod, testDescr)

```


## Pre-process Data: Normalization
```{r pre-process-plot, echo = FALSE, warning = FALSE}

raw.plot = ggplot(trainDescr, aes(alcohol)) + geom_histogram() +
  labs(title = "Original")

scaled.plot = ggplot(trainScaled, aes(alcohol)) + geom_histogram() +
  labs(title = "Scaled")

(raw.plot / scaled.plot) # Using patchwork package

# library(ggpubr)
# ggarrange(raw.plot, scaled.plot,
#           labels = c("Original", "Normalized"),
#           nrow=2, ncol = 1, align = "v")

```


## Tune model hyperparameters with cross-validation
* Used to select best combination of predictors and model parameters

* Estimates model performance (e.g., AUC or r-square) for each candidate model

* Uses a random subset of the training data to train the model and a withheld subset to test


* Select cross validation method: 10-fold repeated cross validation is common

* Define hyperparameter selection method: grid search is the simplest approach

* Define summary measures

* ``trainControl`` command specifies all these parameters in a single statement


## Define training parameters: ``trainControl``
```{r tune, echo=TRUE}

train.control = trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3, # number: number of folds
                              search = "grid", # for tuning hyperparameters
                              classProbs = TRUE, # return probability of prediction
                              savePredictions = "final",
                              summaryFunction = twoClassSummary
                             )

```


## Select models to train
-- Over 200 different models from 50 categories (e.g., Linear regression, boosting, bagging, cost-sensitive learning)

* List of models: http://caret.r-forge.r-project.org/modelList.html

* The ``train`` statement can train any of them

* Here we select three:

    * Logistic regression

    * Support vector machine

    * xgboost, a boosted random forest that performs well in many situations


## Train models and tune hyperparameters with the ``train`` function

* Specify class and predictor variables

* Specify one of the over 200 models (e.g., xgboost)

* Specify the metric, such as ROC

* Include the ``train.control`` object specified earlier



Train models and tune hyperparameters: Logistic regression

* Logistic regression has no tuning parameters

* 10-fold repeated (3 times) cross-validation occurs once 

* Produces a total of 30 instances of model fitting and testing

* Cross validation provides a nearly unbiased estimate of the performance of the model on the held out data

```{r train_glm, cache = TRUE, echo=TRUE, warning=FALSE}

glm.fit = train(x = trainScaled, y = trainClass,
   method = 'glm', metric = "ROC",
   trControl = train.control) 

glm.fit

```


## Train models and tune hyperparameters: Support vector machine

* Linear support vector machines have a single tuning parameter--C

* C (Cost) 

* C = 1000 "hard margin" tends to be sensitive to individual data points and is prone to over fitting

```{r train_svm, cache=TRUE,  echo=TRUE, message=FALSE, warning=FALSE}

grid = expand.grid(C = c(.1, .2, .4, 1, 2, 4))

svm.fit =  train(x = trainScaled, y = trainClass,
  method = "svmLinear", metric = "ROC",
  tuneGrid = grid, # Overrides tuneLength
  tuneLength = 3, # Number of levels of each hyper parameter, unless specified by grid
  trControl = train.control, scaled = TRUE)

plot(svm.fit)

```


## Train model and tune hyperparameters: xgboost
Classification depends on adding outcomes across many trees

Trees are built in sequence to address the errors (residuals) of the previous trees


* Hyperparameters of xgboost

 * nrounds (# Boosting Iterations)--model robustness

 * max_depth (Max Tree Depth)--model complexity

 * eta (Shrinkage)--model robustness

 * gamma (Minimum Loss Reduction)--model complexity

 * colsample_bytree (Subsample Ratio of Columns)--model robustness

 * min_child_weight (Minimum Sum of Instance Weight)--model complexity

 * subsample (Subsample Percentage)--model robustness

**A grid search with 3 levels for each parameter produces 3^7 combinations!**


``tuneLength = 3`` produces 

    - nrounds (# Boosting Iterations) (50 100 150)

    - max_depth (Max Tree Depth) (1, 2, 3)

    - eta (Shrinkage) (.3, .4)

    - gamma (Minimum Loss Reduction) (0)

    - colsample_bytree (Subsample Ratio of Columns) (.6, .8)

    - min_child_weight (Minimum Sum of Instance Weight) (1)

    - subsample (Subsample Percentage) (.50, .75, 1.0)

-- 108 different model combinations each trained and tested 10X3 times

```{r train_xgb, echo=TRUE, cache=TRUE, warning=FALSE, message=FALSE}

xgb.fit = train(x = trainScaled, y = trainClass,
  method = "xgbTree", metric = "ROC",
  tuneLength = 3, # Depends on number of parameters in algorithm
  trControl = train.control, scaled = TRUE)

plot(xgb.fit)

```

  
## Assess performance: Confusion matrix (glm)
Use trained model to predict on the4 unseen test data

```{r assess-glm,  echo=TRUE}

glm.pred = predict(glm.fit, testScaled) 

confusionMatrix(glm.pred, testClass)

```


## Assess performance: Confusion matrix (svm)
```{r assess-svm,  echo=TRUE}

svm.pred = predict(svm.fit, testScaled)

confusionMatrix(svm.pred, testClass)

```


## Assess Performance: Confusion matrix (xgb)
```{r assess-xgb,  echo=TRUE}

xgb.pred = predict(xgb.fit, testScaled)

confusionMatrix(xgb.pred, testClass)

```


## Compare models
```{r compare_boxplot, cache=TRUE, echo=TRUE}

mod.resamps = resamples(list(glm = glm.fit, svm = svm.fit, xgb = xgb.fit))

# dotplot(mod.resamps, metric="ROC")

bwplot(mod.resamps, metric = "ROC")


```


## Assess performance (xgb): ROC plot
```{r assess_ROC, warning=FALSE, echo=FALSE, message= FALSE}

## Use model to generate predictions
xgb.pred = predict(xgb.fit, testScaled, type = "prob")
glm.pred = predict(glm.fit, testScaled, type = "prob")

## Add prediction and observed to test predictors
predicted.wine.df = quality.wine.df %>% slice(-inTrain) %>% 
  cbind(glm.pred.good = glm.pred$good) %>% 
  cbind(xgb.pred.good = xgb.pred$good) %>% 
  cbind(obs = testClass)

## Calculate ROC coordinates and area under curve (AUC)
glm.roc = roc(predictor = predicted.wine.df$glm.pred, 
              response = predicted.wine.df$obs, 
              AUC = TRUE, ci = TRUE)

xgb.roc = roc(predictor = predicted.wine.df$xgb.pred, 
              response = predicted.wine.df$obs, 
              AUC = TRUE, ci = TRUE)


glm.roc$auc
glm.roc$ci


## Plot ROC
xgb_glm.roc.plot = 
ggplot(data = predicted.wine.df, aes(d = obs, m = glm.pred.good)) + 
  geom_roc(labels = FALSE, linealpha = .5, pointalpha = .5) + # Labels show the predictor value
  geom_roc(aes(d = obs, m = xgb.pred.good),
           labels = FALSE, linealpha = .8, pointalpha = .8) + # Labels show the predictor value
   annotate("text", x = .5, y = .475, hjust = 0,
           label = paste("AUC(xbg) =", round(xgb.roc$auc, 2))) +
     annotate("text", x = .5, y = .375, hjust = 0,
           label = paste("AUC(glm) =", round(glm.roc$auc, 2))) +
  labs(title = "Prediction of good and bad wines", 
       subtitle = "Extreme gradient boosting predictions (xgboost)") +
  coord_fixed(ratio = 1) +
  style_roc() 
  
xgb_glm.roc.plot
ggsave("xgb_glm-roc.png", xgb_glm.roc.plot, width = 5, height = 4.5)

```


## Comparing xgboost predictions with observed ratings of wine quality
```{r xgb-pred-plot}

predicted_wine.plot = 
  ggplot(predicted.wine.df, aes(as.factor(quality), xgb.pred.good, colour = goodwine)) + 
  geom_sina(size = .5) +
  labs(title = "Prediction of good and bad wines", 
       subtitle = "Extreme gradient boosting predictions (xgboost)",
       x = "Rated quality",
       y = "Predicted probabilty the wine is good") +
  theme_gray(base_size = 14) +
  theme(legend.position = "bottom") 
predicted_wine.plot

ggsave(filename = "predicted_wine.png", plot = predicted_wine.plot, 
       width = 5, height = 4.5)

```


